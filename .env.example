# =============================================================================
# LLM SERVICE CONFIGURATION
# =============================================================================

# Cloud LLM Configuration (OpenAI)
# Get your API key from https://platform.openai.com/api-keys
BASE_URL=https://api.openai.com/v1
API_KEY=your-openai-api-key-here

# Default Cloud Model (used for complex tasks requiring high accuracy)
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, etc.
CLOUD_MODEL=gpt-4o-mini

# Local LLM Configuration (Ollama)
# Install Ollama from https://ollama.ai
# Run 'ollama serve' to start the local server
# Run 'ollama pull llama3.2' to download the model
LOCAL_BASE_URL=http://localhost:11434/v1
LOCAL_API_KEY=not-needed
LOCAL_MODEL=llama3.2

# =============================================================================
# MODEL SELECTION STRATEGY
# =============================================================================

# Set to "true" to use local models by default for all tasks
# Set to "false" to use cloud models by default
USE_LOCAL_BY_DEFAULT=false

# =============================================================================
# TASK-SPECIFIC MODEL CONFIGURATION
# =============================================================================

# JSON map: task types that should use local models (free)
# Options: true (use local/llama3.2) or false (use cloud/gpt-4o)
# Task types: basic_admin, complex, visualization, document, spreadsheet
# Easy tasks (basic_admin, visualization, document, spreadsheet) use local models for higher profit margins
TASK_USE_LOCAL_MAP={"basic_admin":true,"complex":false,"visualization":true,"document":true,"spreadsheet":true}

# JSON map: specific models for each task type (optional override)
# Leave empty to use default CLOUD_MODEL or LOCAL_MODEL
TASK_MODEL_MAP={}

# =============================================================================
# SANDBOX CONFIGURATION (Docker - replaces E2B for cost savings)
# =============================================================================

# Use Docker sandbox instead of E2B (set to "false" to use E2B)
USE_DOCKER_SANDBOX=true

# Docker image name for sandbox (must match Dockerfile.sandbox)
DOCKER_SANDBOX_IMAGE=ai-sandbox-base

# Docker sandbox timeout in seconds
DOCKER_SANDBOX_TIMEOUT=120

# =============================================================================
# OTHER API KEYS
# =============================================================================

# E2B Code Interpreter API Key (fallback if Docker is disabled)
# Get from https://e2b.dev/docs/getting-started/api-key
E2B_API_KEY=your-e2b-api-key-here

# Stripe Configuration (for payments)
STRIPE_SECRET_KEY=your-stripe-secret-key
STRIPE_PUBLISHABLE_KEY=your-stripe-publishable-key
STRIPE_WEBHOOK_SECRET=your-stripe-webhook-secret

# =============================================================================
# MARKET SCANNER CONFIGURATION
# =============================================================================

# Marketplace URL to scan for job postings
# Set to your target freelance marketplace URL
MARKETPLACE_URL=https://example.com/freelance-jobs

# LLM model to use for job evaluation (must be installed in Ollama)
MARKET_SCAN_MODEL=llama3.2

# Bid amount limits
MIN_BID_AMOUNT=10
MAX_BID_AMOUNT=500

# Scan settings (in seconds)
MARKET_SCAN_PAGE_TIMEOUT=30
MARKET_SCAN_INTERVAL=300  # 5 minutes between scans

# =============================================================================
# LLM REVENUE-BASED OPTIMIZATION
# =============================================================================

# Revenue threshold (in cents) for cloud vs local model selection
# Below this amount: use local model (free)
# Above this amount: use cloud model (best quality)
# Default: 3000 cents = $30
MIN_CLOUD_REVENUE=3000
